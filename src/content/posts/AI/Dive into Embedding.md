---
title: Dive into Embedding
published: 2024-08-21 10:00:00
description: 'The core of Embedding.'
image: ''
tags: ['Embedding', 'LLM']
category: 'LLM'
draft: false 
---

大型语言模型（`LLM`）的热潮背后，所需的资源往往超出中小型企业的承受能力。对这些企业而言，LLM 当前在企业服务（`ToB`）领域的落地场景实际上较为有限，主要集中在替代传统自然语言处理（`NLP`）方案，如命名实体识别（`NER`）和情感分析等。

对于这些传统任务，大模型的优势在于其预训练阶段积累了大量压缩后的知识。这在一定程度上解决了传统人工智能面临的诸多挑战，如同名消歧和灾难性遗忘等问题。同时，它也显著降低了使用、微调和训练人工智能的门槛。

然而，随之而来的问题是本地部署成本高昂，推理速度缓慢。因此，绝大多数公司选择直接使用第三方的 `API` 服务，这种方式不仅经济实惠，而且无需承担维护成本，能够快速高效地开发自己的产品。

但是，`API` 调用存在一个不容忽视的问题——延迟。在互联网产品开发中有一个广为人知的规律：程序中 80% 的时间都在等待网络调用。这句话虽然略显夸张，却不无道理。我们以请求国内某云平台提供的 LLM API 调用测试为例来说明这一点：

```bash
0.01s user 0.02s system 3% cpu 0.684 total
```

-   实际请求 API 和接收响应的时间仅为为 `0.03s`；
-   网络开销+第三方模型推理的时间占用达到 `0.654s`；

对于线上服务来说，任何高延迟的功能都可以被视为不可用或者低可用性，因为用户的使用意愿会随着延迟的上升不断下降，但在产品中引入 `LLM` 确实能让开发成本（时间、需求完成度）大幅下降。于是 `LLM` 中的 `Embedding` 组件开始进入开发者们的视野。

随着 `LLM` 的发展，`Embedding` 模型的质量也在不断提升，现代的 `Embedding` 模型不再是纯粹的词向量（`word2vec`）组合，而是融入 `Attention` 机制后更具可用性的版本。

## 统计语言模型

回顾一下最开始语言预测 `next token` 的方法：比如当前已有的内容是 `"Today's weather is"`，候选词有：`sunny`，`rainy`，`humid`，他们的概率分别是：`0.8`，`0.7`，`0.5`，我们为了使最终输出句子的置信度最高，所以选择 `sunny`。

每个单词概率的来源是采用统计方法，从大量语料中统计得到的。

:::note

1.  单词频率（Term Frequency, TF）： 这是最简单的方法，仅仅计算一个词在文档或整个语料库中出现的次数, `P(word) = count(word) / total_words`;
2.  TF-IDF（Term Frequency-Inverse Document Frequency）： 这种方法不仅考虑词频，还考虑了词在整个文档集中的分布情况。它更多地用于信息检索和文档相关性分析，而不是直接用于计算单词出现的概率;
3.  词袋模型（Bag of Words）： 这是一种简单的文本表示方法，统计每个单词在文档中出现的次数，但不考虑词序;
4.  最大似然估计（Maximum Likelihood Estimation, MLE）： 这种方法直接用词的出现次数除以语料库中的总词数来估计概率。 `P(word) = count(word) / N`， 其中 N 是语料库中的总词数。

:::

不过我们可以很明显感知到这个方法不对：

1.  同样的语句输出的内容必然是固定的；
2.  `next token` 的选择与句子之前的内容没有任何关联；

所以问题变为，我们需要让句子的 `next token`选择是基于上下文语义的。那么就考虑贝叶斯概率，也就是位置 `i` 的 `token` 概率，需要为在 `i-1` 个 `tokens` 组合下的条件概率。

但是这会引入一个新的问题，就是 `i-1` 个 `tokens` 下的条件概率怎么计算，随着句子越来越长，`i-1` 个 `tokens` 下的条件概率会变为一个不可计算值（这也就是大名鼎鼎的 `n-gram` 算法）。

于是有了改进的 `N 阶马尔可夫链`，也就是 `next token` 仅取决于前 `N` 个单词的模型。所以不管句子的长度如何递增，`next token` 的概率必定都是可以计算的。

:::note

然而，N 阶马尔可夫链模型仍然有其局限性。它只能考虑`有限的上下文`，无法捕捉长距离的语义依赖。为了解决这个问题，研究者们开发了更复杂的模型，如循环神经网络（RNN）和长短期记忆网络（LSTM）。这些模型能够在理论上记住任意长度的上下文信息。

但是，`RNN` 和 `LSTM` 在处理长序列时仍然面临梯度消失或梯度爆炸的问题。2017年，`Transformer` 架构的出现标志着语言模型的一次重大突破。`Transformer` 基于自注意力机制，能够并行处理输入序列，有效捕捉长距离依赖关系。这种架构成为了如 `BERT`、`GPT` 等大型语言模型的基础。

现代的 `Embedding` 技术，如 `BERT` 的上下文相关 `Embedding`，不仅将词映射到向量空间，还能根据上下文动态调整表示。这使得模型能够处理`一词多义`等复杂语言现象。大型语言模型（LLM）如 GPT-3 进一步扩展了这些技术，通过海量数据预训练，获得了强大的语言理解和生成能力。

最后，关于"同样的语句输出的内容必然是固定的"这一点，现代语言模型通过引入`随机性（如温度参数调整）`可以产生多样化的输出，解决了早期模型输出固定的问题。这使得语言模型能够生成更加自然、多样的文本。

:::

## word2vec

首先我们需要知道为什么会有 `Embedding` 的出现。简单来说，我们日常的所有输入内容并不能直接被模型接收作为输入，比如`"My dog is so cute"`，即使分词后也不能将其作为某种规则的输入内容。所以就有了将所有输入转换为数字的形式进行表示的想法。但 Embedding 不仅仅是将文本转换为数字，更重要的是它能够捕捉词语间的语义关系，将语义相近的词映射到向量空间中的相近位置。

## Embedding

我们来展开说明下 Embedding 的内部原理：

1.   用户输入 `Query`，使用 `tokenizer` 对其进行分词，此时得到一个个 `token` 组成的数组（注意这里分词涉及不在词表中的 `token` 如何切分）；
2.   将 `token` 与模型内部的词表进行映射，得到每个 `token` 所对应的向量，并按照 `token` 在原句的顺序，组合成一个矩阵，这个矩阵也被称为嵌入矩阵（`embedding matrix`）；
3.   嵌入矩阵输入模型后续的层级中，经过比如注意力矩阵后，再通过全连接层或其他处理，将调整后的矩阵转换为一个固定维度的向量表示。





